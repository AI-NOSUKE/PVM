# -*- coding: utf-8 -*-

from __future__ import annotations

# SPDX-License-Identifier: LicenseRef-PVM-1.2
# Copyright (c) 2025 AI-NOSUKEï¼ˆé€æ˜ãƒšã‚¤ãƒ³ã‚¿ãƒ¼/Phantom Color Painterï¼‰
#
# ä½¿ç”¨è¨±è«¾ï¼ˆè¦ç´„ï¼‰:
# - å€‹äººåˆ©ç”¨ãƒ»ç ”ç©¶åˆ©ç”¨ã¯è‡ªç”±ã«ä½¿ç”¨ãƒ»æ”¹å¤‰ãƒ»å†é…å¸ƒã§ãã¾ã™ã€‚
# - ç¤¾å†…PoCï¼ˆè©•ä¾¡ï¼‰åˆ©ç”¨ã¯ 30 æ—¥ä»¥å†…ã€åŒä¸€çµ„ç¹”æœ€å¤§ 3 åã¾ã§ã€5,000 ãƒ¬ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯ 50MB ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¾ã§ç„¡å„Ÿã§è¨±å®¹ã•ã‚Œã¾ã™ã€‚
# - å•†ç”¨åˆ©ç”¨ã¯å¹´é¡ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¥‘ç´„ãŒå¿…è¦ã§ã™ã€‚è©³ç´°ã¯ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚
# - æœ¬ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯ã€Œç¾çŠ¶ã®ã¾ã¾ã€ï¼ˆAS ISï¼‰æä¾›ã•ã‚Œã¾ã™ã€‚è©³ç´°ã¯ãƒªãƒã‚¸ãƒˆãƒªåŒæ¢±ã® LICENSE ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

__version__ = "1.0.0"
"""
==============================
# PVMã¨ã¯ï¼ˆã¾ãšã¯å…¨ä½“åƒï¼‰
é«˜æ¬¡å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ PCAâ†’ICA ã§äººã«è§£é‡ˆã—ã‚„ã™ã„â€œç‹¬ç«‹è»¸â€ã¸å†æ§‹æˆã—ã€
**cosine** ä¸€è²«ã§å®‰å®šã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹äºŒæ®µæ³•ï¼ˆPCAâ†’ICAâ‘ â†’KMeansâ†’ICAâ‘¡â†’KMeansï¼‰ã€‚
åˆå›ã«ã€Œæ„å‘³åº§æ¨™ï¼ˆè»¸ï¼‰ã¨ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã€ã‚’ baseline_* ã¨ã—ã¦ä¿å­˜ã—ã€ä»¥å¾Œã¯åŒä¸€ç‰©å·®ã—ã§æ¯”è¼ƒï¼ˆãƒ­ãƒƒã‚¯ï¼‰ã€‚
å¿…è¦ãªã‚‰ã‚¢ãƒ³ãƒ­ãƒƒã‚¯ã§â€œæ–°è©±é¡Œã ã‘â€å¸åã—ã€**ç‰ˆè¿½åŠ **ã§åŸºæº–ã‚’å¢—ã‚„ã—ã¾ã™ï¼ˆä¸Šæ›¸ããªã—ï¼‰ã€‚

==============================
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŒ™å‹•ï¼ˆåˆå›â†’2å›ç›®ä»¥é™ï¼‰
- åˆå›: å†…éƒ¨æ¢ç´¢ã®ãƒ™ã‚¹ãƒˆPlanã‚’è‡ªå‹•æ¡ç”¨ã—ã¦ **baseline_{project}/history/v001** ã‚’ä½œæˆ
- 2å›ç›®ä»¥é™ï¼ˆæ—¢å®šï¼‰: æ—¢å­˜åŸºæº–ã«æŠ•å½±ã™ã‚‹ **ğŸ”’ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ­ãƒƒã‚¯**
- 2å›ç›®ä»¥é™ï¼ˆä»»æ„ï¼‰: **ğŸ”“ã‚¢ãƒ³ãƒ­ãƒƒã‚¯**ã§åŸºæº–æº–æ‹ ï¼‹æ–°è©±é¡Œã ã‘è¿½åŠ ã—ã€ç¾ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå´ã« **ç‰ˆè¿½åŠ ä¿å­˜**

â€» baseline ã®é¸æŠå„ªå…ˆåº¦ï¼š`--baseline-from NAME` ï¼ baseline_{project} ï¼ ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹å†…ã®æœ€æ–° baseline_*

==============================
# æœ€çŸ­æ‰‹é †ï¼ˆã‚³ãƒãƒ³ãƒ‰ï¼‰
- åˆå›ï¼ˆè‡ªå‹•æ¡ç”¨ï¼‰                 : `python PVM.py`
- å€™è£œã ã‘è¦‹ã¦æ±ºã‚ãŸã„              : `python PVM.py --show-candidates`
  â†’ è©•ä¾¡è¡¨ã¨ã€Œâ‘¡æœ€çµ‚ã€æ¯”è¼ƒç”¨CSVã‚’å‡ºåŠ›ï¼ˆã“ã®å›ã§ã¯åŸºæº–ã¯ä½œã‚‰ãªã„ï¼‰
  â†’ è‰¯ã•ãã†ãªæ¡ˆã‚’é¸ã‚“ã ã‚‰ `python PVM.py --use-plan N`
     ï¼ˆN ã¯ **k_candidates.csv ã® rank åˆ—ï¼global_plan** ã®ç•ªå·ï¼‰
- 2å›ç›®ä»¥é™ï¼ˆæ—¢å®šï¼ãƒ­ãƒƒã‚¯ï¼‰        : `python PVM.py`
- 2å›ç›®ä»¥é™ï¼ˆæŸ”è»Ÿé©ç”¨ï¼ã‚¢ãƒ³ãƒ­ãƒƒã‚¯ï¼‰: `python PVM.py --unlock`

ï¼ˆãƒ­ã‚°ã®äºˆå‘Šè¨˜å·ï¼šğŸ§­ åˆå› / ğŸ”’ ãƒ­ãƒƒã‚¯äºˆå®š / ğŸ”“ æŸ”è»Ÿé©ç”¨äºˆå®šï¼‰

==============================
# å€™è£œã®è¦‹æ–¹ï¼ˆâ‘¡æœ€çµ‚ã§æ¯”è¼ƒã—ã¦é¸ã¶ï¼‰
`--show-candidates` ã¯æ¬¡ã®3ã¤ã‚’å‡ºåŠ›ã—ã¾ã™ï¼š
1) **k_candidates.csv** â€¦ ICAâ‘ ã® **d Ã— K** å…¨å€™è£œã®è©•ä¾¡ï¼ˆsilâ†‘/CHâ†‘/1/(1+DB)â†‘ ã®**å¹³å‡ãƒ©ãƒ³ã‚¯**ï¼‰
2) **k_candidates_stage2.csv** â€¦ **d\***å›ºå®šãƒ»**Kã ã‘**å¤‰ãˆãŸTOP5ã® **â‘¡æœ€çµ‚**è©•ä¾¡
3) **k_candidates_assignments.csv** â€¦ **å…¨ãƒ†ã‚­ã‚¹ãƒˆ Ã— TOP5å€™è£œã® â€œâ‘¡æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿IDâ€**ï¼ˆæ„å‘³ã®åˆ‡ã‚Œæ–¹ã‚’æ¯”è¼ƒï¼‰
â†’ åŸºæœ¬ã¯ 2) ã¨ 3) ã‚’è¦‹ã¦ã€Œåˆ†ã‹ã‚Œæ–¹ãŒè‰¯ã„ Kã€ã‚’æ±ºã‚ã€1) ã® **rankï¼ˆglobal_planï¼‰** ç•ªå·ã‚’ `--use-plan N` ã«æŒ‡å®š

â€» d\* ã¯ 1) ã®å¹³å‡ãƒ©ãƒ³ã‚¯ã‚’ **d ã”ã¨**ã«é›†è¨ˆã—æœ€è‰¯ã® d ã‚’è‡ªå‹•é¸å®šï¼ˆå®Ÿè£…ã«ä¸€è‡´ï¼‰

==============================
# å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆä¿¯ç°ï¼‰
Embedding (Ruri) â†’ PCA â†’ ICAâ‘  â†’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°â‘ ï¼ˆå€™è£œæ¢ç´¢ï¼‰
                    â†“ï¼ˆPlané¸æŠï¼šd*, K*ï¼‰
                  ICAâ‘¡ï¼ˆ= min(K*-1, d*)ï¼‰ â†’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°â‘¡ï¼ˆæœ€çµ‚ï¼‰
                                â†“
             åŸºæº–ä¿å­˜ï¼ˆåˆå›ï¼‰ / åŸºæº–é©ç”¨ï¼ˆãƒ­ãƒƒã‚¯ï¼‰/ æŸ”è»Ÿé©ç”¨ï¼ˆã‚¢ãƒ³ãƒ­ãƒƒã‚¯ï¼‰

==============================
# ã‚¢ãƒ³ãƒ­ãƒƒã‚¯ï¼ˆæŸ”è»Ÿé©ç”¨ï¼‰
- æ—¢å­˜åŸºæº–ã«æŠ•å½±ã—ã€**åŸºæº–ã‹ã‚‰é ã„é›†åˆã ã‘**ã‚’ outlierï¼ˆè·é›¢åˆ†ä½ç‚¹ qï¼‰ã¨åˆ¤æ–­
- ãã®é›†åˆã®ä¸­ã§æœ€å¤§ `add_k` å€‹ã®æ–°ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿½åŠ 
- è¿½åŠ å¾Œã®åŸºæº–ã¯ **baseline_{ç¾åœ¨project}** ã« **history ç‰ˆè¿½åŠ **ï¼ˆå…ƒã®åŸºæº–ã¯ãã®ã¾ã¾ï¼‰
- ä»£è¡¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š`--unlock-q 0.90`ï¼ˆ0<q<1ï¼‰ / `--unlock-add-k 2`

==============================
# å‡ºåŠ›ï¼ˆæ¯å›çµ±ä¸€ï¼‰
- çµæœã‚¹ã‚³ã‚¢.csv        : [id, text, IC1..ICmï¼ˆICAâ‘¡ï¼‰, cluster, dist]ï¼ˆã‚¢ãƒ³ãƒ­ãƒƒã‚¯æ™‚ã¯ãƒ•ãƒ©ã‚°åˆ—ä»˜ãï¼‰
- çµæœãƒ¬ãƒãƒ¼ãƒˆ.json     : å®Ÿè¡Œè¨­å®š/Plan/æ¬¡å…ƒæ•°/K/metric_family/ç’°å¢ƒ/ä½¿ç”¨baseline/ãƒ¢ãƒ¼ãƒ‰
- AI_å‘½åä¾é ¼.md        : ã‚¯ãƒ©ã‚¹ã‚¿åç§°ä»˜ã‘ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆä»£è¡¨æ–‡ï¼‰
- k_candidates.csv       : â‘ å…¨å€™è£œï¼ˆdÃ—Kï¼‰ã®è©•ä¾¡ä¸€è¦§ï¼ˆ**rankï¼global_plan** ã‚’æ¡ç•ªï¼‰
- k_candidates_stage2.csv: **d\***å›ºå®šã§ K ã ã‘æ¯”è¼ƒã—ãŸTOP5ã® **â‘¡æœ€çµ‚**è©•ä¾¡
- k_candidates_assignments.csv: å…¨ãƒ†ã‚­ã‚¹ãƒˆ Ã— TOP5å€™è£œã® **â‘¡æœ€çµ‚**ã‚¯ãƒ©ã‚¹ã‚¿ID

==============================
# è©•ä¾¡ã¨è·é›¢ï¼ˆcosineã§ä¸€è²«ï¼‰
- æŒ‡æ¨™ï¼šsilï¼ˆâ†‘ï¼‰/ CHï¼ˆâ†‘ï¼‰/ DB_inv=1/(1+DB)ï¼ˆâ†‘ï¼‰ã®å¹³å‡ãƒ©ãƒ³ã‚¯ã§ç·åˆè©•ä¾¡
- å‰²å½“ï¼šL2æ­£è¦åŒ–ãƒ™ã‚¯ãƒˆãƒ«ã® **cosineè·é›¢**ï¼ˆã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã‚‚æ­£è¦åŒ–ï¼‰

==============================
# å®Ÿè£…ãƒ¡ãƒ¢ï¼ˆå®‰å®šåŒ–ï¼‰
- FastICAï¼ˆâ‘ â‘¡ï¼‰: `max_iter=5000`, `tol=1e-4`ï¼ˆåæŸå®‰å®šåŒ–ï¼‰ã€‚åæŸã—ãªã„å ´åˆã¯ã‚·ãƒ¼ãƒ‰å¤‰æ›´ã‚„æ¬¡å…ƒ-1ã§å†è©¦è¡Œ
- Embedding ã¯ Ruriï¼ˆcl-nagoya/ruri-v3-310mï¼‰æƒ³å®šã€‚ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã¯ baseline ã¨ä¸ä¸€è‡´ã‚’è­¦å‘Šï¼ˆæ¯”è¼ƒå³å¯†æ€§ãŒä½ä¸‹ï¼‰

==============================
# æ³¨æ„
- â€œå®Œå…¨å†æ¢ç´¢ã®ä¸Šæ›¸ãâ€ã¯æä¾›ã—ã¾ã›ã‚“ã€‚åˆå›ã‹ã‚‰ä½œã‚Šç›´ã™å ´åˆã¯ `PVMresult` ã‚’æ¶ˆã™ã‹ã€`--project` ã‚’å¤‰ãˆã¦ãã ã•ã„ã€‚
- ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãŒå°‘ãªã„å ´åˆï¼ˆä¾‹: n < 30ï¼‰ã¯æ¢ç´¢ãŒç²—ããªã‚Šã¾ã™ï¼ˆãƒ­ã‚°ã§è­¦å‘Šï¼‰ã€‚
- å†ç¾æ€§ãŒé‡è¦ãªã‚‰ `--random_state` ã‚’å›ºå®šã—ã¦ãã ã•ã„ï¼ˆè§£æãƒ­ã‚¸ãƒƒã‚¯ã«å½±éŸ¿ã¯ãªã„ãŒã‚¯ãƒ©ã‚¹ã‚¿åˆæœŸå€¤ã¯é–¢ä¸ï¼‰ã€‚
"""


import os, sys, json, argparse, logging, re, unicodedata, hashlib, platform
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, FastICA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.metrics.pairwise import cosine_distances

# tqdmï¼ˆç„¡ã‘ã‚Œã°ãƒ€ãƒŸãƒ¼ï¼‰
try:
    from tqdm import tqdm
except Exception:
    def tqdm(x, **k): return x  # fallbackï¼ˆé€²æ—ãƒãƒ¼ç„¡ã—ï¼‰

SCRIPT_VERSION = "PVM-cosine-two-stage-1.6 (stage2-candidates-added)"

# ----------------- ãƒ­ã‚° -----------------
def setup_logging(level: str = "INFO"):
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s [%(levelname)s] [CHIBI] %(message)s",
        datefmt="%H:%M:%S",
    )
log = logging.getLogger(__name__)

# ----------------- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¢ç´¢ï¼ˆå±¥æ­´ã‚ã‚Šã«é™å®šï¼‰ -----------------
def find_latest_baseline_project(result_root: Path) -> Optional[str]:
    ok = []
    for p in result_root.glob("baseline_*"):
        hist = p / "history"
        if hist.exists() and any(hist.glob("v*")):
            ok.append(p)
    if not ok:
        return None
    ok.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return ok[0].name.replace("baseline_", "", 1)

# -------------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ --------------
def normalize_text(s: Any) -> str:
    s = "" if s is None else str(s)
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\n", " ").replace("\r", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def autodetect_input() -> Tuple[Path, str]:
    here = Path(".")
    for name in ["å…¥åŠ›.xlsx", "å…¥åŠ›.csv"]:
        p = here / name
        if p.exists():
            return p.resolve(), p.suffix.lstrip(".").lower()
    cands = sorted(list(here.glob("*.xlsx")) + list(here.glob("*.csv")),
                   key=lambda p: p.stat().st_mtime, reverse=True)
    if not cands:
        raise FileNotFoundError("å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå…¥åŠ›.xlsx / å…¥åŠ›.csv / *.xlsx / *.csvï¼‰")
    return cands[0].resolve(), cands[0].suffix.lstrip(".").lower()

def read_table(path: Path, ext: str) -> pd.DataFrame:
    if ext == "xlsx":
        try:
            return pd.read_excel(path)
        except ImportError:
            log.error("Excelèª­ã¿è¾¼ã¿ã«ã¯ openpyxl ãŒå¿…è¦ã§ã™ã€‚pip install openpyxl")
            raise
    try:
        return pd.read_csv(path, encoding="utf-8")
    except UnicodeDecodeError:
        try:
            return pd.read_csv(path, encoding="utf-8-sig")
        except UnicodeDecodeError:
            log.warning("UTF-8ç³»ã§èª­ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚CP932ã‚’è©¦ã—ã¾ã™ã€‚")
            return pd.read_csv(path, encoding="cp932")

def autodetect_columns(df: pd.DataFrame, text_col: Optional[str], id_col: Optional[str]) -> Tuple[str, Optional[str]]:
    cols = list(df.columns)
    lowers = {c.lower(): c for c in cols}
    if not text_col:
        for c in ["text", "ãƒ†ã‚­ã‚¹ãƒˆ", "æœ¬æ–‡", "content", "sentence", "æ„å‘³", "comment", "body"]:
            if c.lower() in lowers:
                text_col = lowers[c.lower()]
                break
        if not text_col and cols:
            lengths = {c: df[c].astype(str).str.len().mean() for c in cols}
            text_col = max(lengths, key=lengths.get)
    if not text_col or text_col not in df.columns:
        raise ValueError("ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®è‡ªå‹•æ¤œå‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚--text_col ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    if id_col and id_col not in df.columns:
        id_col = None
    return text_col, id_col

def get_project_name(args_tag: Optional[str], inpath: Path) -> str:
    if args_tag:
        return args_tag
    base = inpath.stem
    return re.sub(r"[^0-9A-Za-zä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³_-]+", "", base or "PVM")

def sha256_of_file(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def l2_normalize(X: np.ndarray, eps: float = 1e-9) -> np.ndarray:
    nrm = np.linalg.norm(X, axis=1, keepdims=True) + eps
    return X / nrm

# ----------------- Embeddingï¼ˆRuriï¼‰ -----------------
def ensure_ruri():
    try:
        import torch  # noqa
        from transformers import AutoTokenizer, AutoModel  # noqa
        return True
    except Exception as e:
        log.error("RuriåŸ‹ã‚è¾¼ã¿ã«ã¯ torch / transformers ãŒå¿…è¦ã§ã™ã€‚")
        log.error("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¾‹: pip install torch transformers")
        log.error("è©³ç´°: %s", e)
        return False

def compute_embeddings(texts: List[str], model_name: str, batch: int, max_len: int) -> Tuple[np.ndarray, str]:
    import torch
    from transformers import AutoTokenizer, AutoModel
    device = "cuda" if torch.cuda.is_available() else "cpu"
    log.info("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: %s", model_name)
    tok = AutoTokenizer.from_pretrained(model_name)
    mdl = AutoModel.from_pretrained(model_name).to(device).eval()
    reps = []
    truncated_total = 0
    bar_disable = not sys.stdout.isatty()
    log.info("Embeddingé–‹å§‹: total=%d, batch=%d, max_len=%d, device=%s", len(texts), batch, max_len, device)
    with torch.inference_mode():
        with tqdm(total=len(texts), desc="Embedding", unit="text", disable=bar_disable) as pbar:
            for i in range(0, len(texts), batch):
                batch_text = texts[i:i+batch]
                enc = tok(batch_text, padding=True, truncation=True,
                          max_length=max_len, return_tensors="pt", return_length=True)
                if "length" in enc:
                    try:
                        truncated_total += int((enc["length"] > max_len).sum().item())
                    except Exception:
                        pass
                enc = {k: v.to(device) for k, v in enc.items()
                       if k in ("input_ids", "attention_mask", "token_type_ids")}
                out = mdl(**enc).last_hidden_state
                mask = enc["attention_mask"].unsqueeze(-1)
                emb = (out * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)
                reps.append(emb.detach().cpu().numpy())
                pbar.update(len(batch_text))
    if truncated_total > 0:
        log.warning("Tokenizerã§ %d æ–‡ãŒ max_len=%d ã‚’è¶…ãˆãŸãŸã‚ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸã€‚", truncated_total, max_len)
    X = np.vstack(reps).astype(np.float32)
    log.info("åŸ‹ã‚è¾¼ã¿å®Œäº†ï¼ˆ%dæ–‡, device=%sï¼‰", len(texts), device)
    return X, device

# ----------------- è©•ä¾¡ï¼ˆcosineä¸€è²«ï¼‰ -----------------
def eval_cluster(X_normalized: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
    out = {}
    try:
        out["sil"] = float(silhouette_score(X_normalized, labels, metric="cosine"))
    except Exception:
        out["sil"] = -1.0
    try:
        out["ch"] = float(calinski_harabasz_score(X_normalized, labels))
    except Exception:
        out["ch"] = 0.0
    try:
        db = float(davies_bouldin_score(X_normalized, labels))
        out["db"] = db
        out["db_inv"] = 1.0/(1.0+db)
    except Exception:
        out["db"] = 10.0
        out["db_inv"] = 0.0
    return out

# ----------------- ICAãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆäº’æ›ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ -----------------
def _fastica_safe(n_components: int, rs: int) -> FastICA:
    """sklearnã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å·®ã‚’å¸åï¼ˆwhiten='unit-variance'â†’ç„¡ã„å ´åˆã¯ Trueï¼‰"""
    try:
        return FastICA(n_components=n_components, random_state=rs,
                       whiten="unit-variance", max_iter=5000, tol=1e-4)
    except TypeError:
        return FastICA(n_components=n_components, random_state=rs,
                       whiten=True, max_iter=5000, tol=1e-4)

# ----------------- è»¸ï¼ˆPCA â†’ ICAâ‘  â†’ ICAâ‘¡ï¼‰ -----------------
def fit_stage1_axes(X: np.ndarray, pca_var: float, ica1_dim: int, random_state: int = 42) -> Tuple[Dict[str, Any], np.ndarray]:
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X)
    max_pcs = min(Xs.shape[0]-1, Xs.shape[1])
    pca = PCA(n_components=max_pcs, random_state=random_state)
    Xp = pca.fit_transform(Xs)
    cumsum = np.cumsum(pca.explained_variance_ratio_)
    n_pcs = int(np.searchsorted(cumsum, pca_var) + 1)
    n_pcs = max(2, min(n_pcs, Xp.shape[1]))
    ica1_dim = max(2, min(ica1_dim, n_pcs))
    try:
        ica1 = _fastica_safe(ica1_dim, random_state)
        Xi1 = ica1.fit_transform(Xp[:, :n_pcs])
    except Exception:
        try:
            ica1 = _fastica_safe(ica1_dim, random_state + 1)
            Xi1 = ica1.fit_transform(Xp[:, :n_pcs])
        except Exception:
            ica1_dim2 = max(2, ica1_dim - 1)
            log.warning("ICAâ‘ ãŒåæŸã—ãªã„ãŸã‚ n_components ã‚’ %dâ†’%d ã«ä¸‹ã’ã¦å†è©¦è¡Œã—ã¾ã™ã€‚", ica1_dim, ica1_dim2)
            ica1 = _fastica_safe(ica1_dim2, random_state + 2)
            Xi1 = ica1.fit_transform(Xp[:, :n_pcs])
    meta = dict(
        scaler_mean=scaler.mean_.tolist(),
        scaler_scale=scaler.scale_.tolist(),
        pca_components=pca.components_.tolist(),
        pca_mean=pca.mean_.tolist(),
        pca_n_components=n_pcs,
        ica1_components=ica1.components_.tolist(),
        ica1_n_components=int(ica1.n_components),
        embed_dim=int(X.shape[1]),
    )
    return meta, Xi1.astype(np.float32)

def apply_stage1_axes(X: np.ndarray, meta: Dict[str, Any]) -> np.ndarray:
    scaler_mean = np.array(meta["scaler_mean"])
    scaler_scale = np.array(meta["scaler_scale"])
    pca_components = np.array(meta["pca_components"])
    pca_mean = np.array(meta["pca_mean"])
    n_pcs = int(meta["pca_n_components"])
    ica1_components = np.array(meta["ica1_components"])
    if X.shape[1] != int(meta.get("embed_dim", X.shape[1])):
        raise ValueError("åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒãŒåŸºæº–ã¨ä¸ä¸€è‡´ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚„å‰å‡¦ç†ãŒå¤‰ã‚ã£ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    Xs = (X - scaler_mean) / np.where(scaler_scale==0.0, 1.0, scaler_scale)
    Xp = (Xs - pca_mean) @ pca_components.T
    Xp_sel = Xp[:, :n_pcs]
    Xi1 = Xp_sel @ ica1_components.T
    return Xi1.astype(np.float32)

def fit_stage2_axes(Xi1: np.ndarray, k: int, random_state: int = 42) -> Tuple[Dict[str, Any], np.ndarray]:
    ic2 = max(1, min(k - 1, Xi1.shape[1]))
    try:
        ica2 = _fastica_safe(ic2, random_state)
        Xi2 = ica2.fit_transform(Xi1)
    except Exception:
        try:
            ica2 = _fastica_safe(ic2, random_state + 1)
            Xi2 = ica2.fit_transform(Xi1)
        except Exception:
            ic2b = max(1, ic2 - 1)
            log.warning("ICAâ‘¡ãŒåæŸã—ãªã„ãŸã‚ n_components ã‚’ %dâ†’%d ã«ä¸‹ã’ã¦å†è©¦è¡Œã—ã¾ã™ã€‚", ic2, ic2b)
            ica2 = _fastica_safe(ic2b, random_state + 2)
            Xi2 = ica2.fit_transform(Xi1)
    meta2 = dict(
        ica2_components=ica2.components_.tolist(),
        ica2_n_components=int(ica2.n_components),
    )
    return meta2, Xi2.astype(np.float32)

def apply_stage2_axes(Xi1: np.ndarray, meta2: Dict[str, Any]) -> np.ndarray:
    ica2_components = np.array(meta2["ica2_components"])
    Xi2 = Xi1 @ ica2_components.T
    return Xi2.astype(np.float32)

# ----------------- å€™è£œæ¢ç´¢ï¼ˆICAâ‘  dim Ã— Kï¼‰ -----------------
def propose_ica1_dims_via_pca(X: np.ndarray, pca_var: float) -> List[int]:
    scaler = StandardScaler().fit(X)
    Xs = scaler.transform(X)
    max_pcs = min(Xs.shape[0]-1, Xs.shape[1])
    pca = PCA(n_components=max_pcs).fit(Xs)
    dims = set()
    for v in [0.80, 0.85, 0.90, 0.95]:
        n = int(np.searchsorted(np.cumsum(pca.explained_variance_ratio_), v) + 1)
        if 2 <= n <= max_pcs:
            dims.add(n)
    for d in [16, 24, 32]:
        if d <= max_pcs:
            dims.add(d)
    dims = sorted([d for d in dims if 8 <= d <= min(64, max_pcs)])
    if len(dims) > 5:
        idx = np.linspace(0, len(dims)-1, 5).round().astype(int)
        dims = [dims[i] for i in idx]
    return dims

def rank_candidates_inplace(cands: List[Dict[str, Any]]) -> None:
    if not cands:
        return
    def ranks(vals: List[float], reverse=True):
        order = np.argsort(vals)[::-1] if reverse else np.argsort(vals)
        r = np.empty_like(order, dtype=float)
        r[order] = np.arange(1, len(vals)+1)
        return r
    sil_r = ranks([c["sil"] for c in cands], reverse=True)
    ch_r  = ranks([c["ch"]  for c in cands], reverse=True)
    dbi_r = ranks([c["db_inv"] for c in cands], reverse=True)
    for i, c in enumerate(cands):
        c["rank_mean"] = float((sil_r[i] + ch_r[i] + dbi_r[i]) / 3.0)
    cands.sort(key=lambda c: (c["rank_mean"], -c["sil"]))

def explore_candidates(X: np.ndarray, k_min: int, k_max: int, pca_var: float, random_state: int) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    n = len(X)
    if n < 3:
        raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒ3ä»¶æœªæº€ã§ã™ã€‚æœ€ä½3ä»¶ä»¥ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦ã§ã™ã€‚")
    k_max_eff = max(2, min(k_max, n - 1))
    if k_max_eff < k_max:
        log.warning("ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã®éƒ½åˆã§ k_max ã‚’ %d â†’ %d ã«èª¿æ•´ã—ã¾ã—ãŸã€‚", k_max, k_max_eff)
    k_max = k_max_eff
    dims = propose_ica1_dims_via_pca(X, pca_var)
    all_cands: List[Dict[str, Any]] = []
    for d in dims:
        meta1, Xi1 = fit_stage1_axes(X, pca_var=pca_var, ica1_dim=d, random_state=random_state)
        Xi1_norm = l2_normalize(Xi1)
        for k in range(max(2, k_min), max(k_min, k_max)+1):
            if k >= len(Xi1_norm):
                continue
            km = KMeans(n_clusters=k, n_init=10, random_state=random_state)
            labels1 = km.fit_predict(Xi1_norm)
            met = eval_cluster(Xi1_norm, labels1)
            all_cands.append(dict(ica1_dim=int(meta1["ica1_n_components"]), k=k, **met))
    rank_candidates_inplace(all_cands)
    best = all_cands[0] if all_cands else None
    top5 = all_cands[:5]
    return all_cands, dict(best=best, top5=top5, dims=dims)

# ---- Stage2ç”¨è£œåŠ©ï¼šd*ã®é¸å®šï¼ˆå¹³å‡ãƒ©ãƒ³ã‚¯æœ€è‰¯ï¼‰ã¨TOP5æŠ½å‡º ----
def choose_best_d_by_rank(cands: List[Dict[str, Any]]) -> int:
    df = pd.DataFrame(cands)
    grp = df.groupby("ica1_dim")["rank_mean"].mean().sort_values()
    return int(grp.index[0])

def pick_top5_for_d(cands: List[Dict[str, Any]], d_star: int) -> List[Dict[str, Any]]:
    filt = [c for c in cands if int(c["ica1_dim"]) == int(d_star)]
    filt.sort(key=lambda c: (c["rank_mean"], -c["sil"]))
    return filt[:5]

# ----------------- å‡ºåŠ› -----------------
def export_candidates(outdir: Path, cands: List[Dict[str, Any]]):
    outdir.mkdir(parents=True, exist_ok=True)
    if not cands:
        return
    df = pd.DataFrame(cands)
    df.insert(0, "rank", np.arange(1, len(df)+1))
    (outdir / "k_candidates.csv").write_text("", encoding="utf-8")  # ensure parent exists on Windows edge cases
    df.to_csv(outdir / "k_candidates.csv", index=False, encoding="utf-8-sig")
    log.info("å€™è£œä¸€è¦§ã‚’å‡ºåŠ›: %s", outdir / "k_candidates.csv")

def export_run_csv(outdir: Path, df_src: pd.DataFrame, df_cols_keep: List[str],
                   Xi2: np.ndarray, labels: np.ndarray, dists: np.ndarray,
                   max_ic_cols: Optional[int], extra_cols: Optional[Dict[str, Any]] = None):
    outdir.mkdir(parents=True, exist_ok=True)
    res = df_src[df_cols_keep].copy()
    ic_total = Xi2.shape[1]
    show_ic = ic_total if not max_ic_cols else min(ic_total, int(max_ic_cols))
    for i in range(show_ic):
        res[f"IC{i+1}"] = Xi2[:, i]
    res["cluster"] = labels.astype(int)
    res["dist"] = dists.astype(float)
    if extra_cols:
        for k, v in extra_cols.items():
            res[k] = v
    outp = outdir / "çµæœã‚¹ã‚³ã‚¢.csv"
    res.to_csv(outp, index=False, encoding="utf-8-sig")
    log.info("ã‚¹ã‚³ã‚¢å‡ºåŠ›: %s", outp)

def export_report(outdir: Path, report: Dict[str, Any]):
    outdir.mkdir(parents=True, exist_ok=True)
    with open(outdir / "çµæœãƒ¬ãƒãƒ¼ãƒˆ.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    log.info("ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: %s", outdir / "çµæœãƒ¬ãƒãƒ¼ãƒˆ.json")


def export_naming_prompt(outdir: Path,
                         df_src: pd.DataFrame,
                         text_col: str,
                         labels: np.ndarray,
                         dists: np.ndarray,
                         top_n: int = 5,
                         include_dist: bool = True) -> None:
    """
    ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã« dist æ˜‡é † ä¸Šä½Nä»¶ï¼ˆä¸­å¿ƒã«è¿‘ã„ä»£è¡¨æ–‡ï¼‰ã‚’ä¸¦ã¹ã€
    ã‚¯ãƒ©ã‚¹ã‚¿å‘½åä¾é ¼ã® Markdown ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
    """
    # --- resolve effective text column ---
    eff_text_col = text_col
    if eff_text_col not in df_src.columns:
        # Try common fallbacks
        fallbacks = ['æ„å‘³', 'text', 'ãƒ†ã‚­ã‚¹ãƒˆ', 'æœ¬æ–‡', 'å†…å®¹', 'æ–‡', 'Text', 'TEXT', 'body', 'sentence']
        for c in fallbacks:
            if c in df_src.columns:
                eff_text_col = c
                break
        else:
            # pick the first object/string dtype column as a last resort
            try:
                eff_text_col = next(col for col in df_src.columns if df_src[col].dtype == 'object')
            except StopIteration:
                # if nothing suitable, raise a clear error
                raise KeyError(f"Text column '{text_col}' not found and no suitable fallback column detected. Available cols: {list(df_src.columns)}")
        try:
            log.warning("text_col '%s' not found; using '%s' instead.", text_col, eff_text_col)
        except Exception:
            pass
    outdir.mkdir(parents=True, exist_ok=True)
    L = np.asarray(labels).astype(int)
    D = np.asarray(dists).astype(float)

    lines = []
    lines.append("# AIå‘½åä¾é ¼ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿åç§°ã¥ã‘ç‰¹åŒ–ãƒ»distä½µè¨˜ï¼‰")
    lines.append("")
    lines.append("## ä¾é ¼")
    lines.append("å„ã‚¯ãƒ©ã‚¹ã‚¿ã«ã¤ã„ã¦ã€ä»£è¡¨æ–‡ï¼ˆdistãŒå°ã•ã„é † ä¸Šä½Nä»¶ï¼‰ã‚’æ ¹æ‹ ã«ã€å®Ÿå‹™ã§é€šã˜ã‚‹çŸ­ã„ã‚¯ãƒ©ã‚¹ã‚¿åç§°ã‚’3æ¡ˆãšã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚")
    lines.append("å„æ¡ˆã«ã¯ä¸€è¡Œã®æ ¹æ‹ è¦ç´„ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚å¿…è¦ã«å¿œã˜ã¦â€œå¹…ã®æ³¨è¨˜â€ã‚‚1è¡Œã§ã€‚")
    lines.append("")
    lines.append("## å‰æï¼ˆé‡è¦ï¼‰")
    lines.append("- dist ã¯æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆICAâ‘¡å¾Œã®KMeansï¼‰ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¨ã®ã‚³ã‚µã‚¤ãƒ³è·é›¢ã€‚å°ã•ã„ã»ã©ä¸­å¿ƒçš„ï¼å…¸å‹ã€‚")
    lines.append(f"- ã¾ãšã¯ä¸Šä½N={top_n} ã®ä»£è¡¨æ–‡ã ã‘ã§å‘½åæ¡ˆã‚’ä½œæˆã€‚ä½™è£•ãŒã‚ã‚Œã° dist ãŒå¤§ãã„æ–‡ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿å†… q90 ä»¥ä¸Šï¼‰ã‚‚è¦‹ã¦å¹…ã‚’è£œè¶³ã€‚")
    lines.append("")
    lines.append("## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¾‹ï¼‰")
    lines.append("- **Cluster k**")
    lines.append("  - **åç§°æ¡ˆAï¼ˆæœ€æœ‰åŠ›ï¼‰**ï¼šï¼¿ï¼¿ï¼¿ï¼¿")
    lines.append("    - è¦ç´„ï¼šï¼¿ï¼¿ï¼¿ï¼¿ï¼ˆä»£è¡¨æ–‡ã®å…±é€šç‚¹ã‚’ä¸€è¡Œã§ï¼‰")
    lines.append("  - åç§°æ¡ˆBï¼šï¼¿ï¼¿ï¼¿ï¼¿ï¼ˆä¸€è¡Œè¦ç´„ï¼‰")
    lines.append("  - åç§°æ¡ˆCï¼šï¼¿ï¼¿ï¼¿ï¼¿ï¼ˆä¸€è¡Œè¦ç´„ï¼‰")
    lines.append("  - ï¼ˆä»»æ„ï¼‰å¹…ã®æ³¨è¨˜ï¼šã“ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«ã¯ ï¼¿ï¼¿ï¼¿ï¼¿ ã®å‘¨è¾ºä¾‹ã‚‚å«ã¾ã‚Œã‚‹")
    lines.append("")
    lines.append("## ä»£è¡¨æ–‡ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ï¼šdist æ˜‡é †ï¼‰")

    for k in sorted(np.unique(L)):
        idx = np.where(L == k)[0]
        if len(idx) == 0:
            continue
        order = idx[np.argsort(D[idx])]
        pick = order[:top_n]
        lines.append(f"> ### Cluster {k}ï¼ˆä¸Šä½{min(top_n, len(pick))}ï¼‰")
        for j in pick:
            t = normalize_text(df_src.iloc[j][eff_text_col])
            if include_dist:
                lines.append(f"> - `dist={D[j]:.4f}`ï¼š{t}")
            else:
                lines.append(f"> - {t}")
        lines.append("")

    with open(outdir / "AI_å‘½åä¾é ¼.md", "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    log.info("å‘½åä¾é ¼ãƒ†ãƒ³ãƒ—ãƒ¬å‡ºåŠ›: %s", outdir / "AI_å‘½åä¾é ¼.md")

# ---- å€™è£œï¼ˆStage2æœ€çµ‚ï¼‰ã®è£œåŠ©å‡ºåŠ› ----
def export_stage2_selection(run_dir: Path, df_src: pd.DataFrame, keep_cols: List[str],
                            X: np.ndarray, pca_var: float, d_star: int,
                            plans: List[Dict[str, Any]], random_state: int,
                            max_ic_cols: Optional[int]) -> None:
    """
    d* ã‚’å›ºå®šã—ã€å„ Plan ã® K ã§ ICAâ‘¡â†’æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œã£ã¦å®Ÿå‰²å½“ï¼ˆassignmentsï¼‰ã¨
    æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆsil/CH/DB_invï¼‰ã‚’ CSV å‡ºåŠ›ã™ã‚‹ã€‚
    """
    # Stage1 ã‚’ d* ã§å›ºå®šã—ã¦ä¸€åº¦ã ã‘
    meta1, Xi1 = fit_stage1_axes(X, pca_var=pca_var, ica1_dim=d_star, random_state=random_state)

    # æº–å‚™
    assign_df = df_src[keep_cols].copy()
    stage2_rows = []

    for idx, c in enumerate(plans, start=1):
        k = int(c["k"])
        # ICAâ‘¡ & æœ€çµ‚å‰²å½“
        meta2, Xi2 = fit_stage2_axes(Xi1, k, random_state=random_state)
        Xi2_norm = l2_normalize(Xi2)
        km = KMeans(n_clusters=k, n_init=20, random_state=random_state)
        labels = km.fit_predict(Xi2_norm)
        C_norm = l2_normalize(km.cluster_centers_.astype(np.float32))
        D = cosine_distances(Xi2_norm, C_norm)
        dmin = D[np.arange(len(D)), labels]

        # æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆâ‘¡ãƒ™ãƒ¼ã‚¹ï¼‰
        met = eval_cluster(Xi2_norm, labels)
        stage2_rows.append(dict(
            plan=idx, ica1_dim=d_star, ic2_dim=int(meta2["ica2_n_components"]), k=k,
            sil=met["sil"], ch=met["ch"], db_inv=met["db_inv"]
        ))

        # å‰²å½“åˆ—ï¼ˆå€™è£œi_Kkï¼‰
        col_name = f"cand{idx}_K{k}"
        assign_df[col_name] = labels.astype(int)

        # ï¼ˆå¿…è¦ãªã‚‰è·é›¢åˆ—ã‚‚ï¼‰ä¾‹: assign_df[f"dist{idx}_K{k}"] = dmin.astype(float)

    # ãƒ©ãƒ³ã‚¯ä»˜ã‘ï¼ˆsil/CH/DB_invã®å¹³å‡ãƒ©ãƒ³ã‚¯ï¼‰
    s2 = pd.DataFrame(stage2_rows)
    def ranks(vals, reverse=True):
        order = np.argsort(vals)[::-1] if reverse else np.argsort(vals)
        r = np.empty_like(order, dtype=float)
        r[order] = np.arange(1, len(vals)+1)
        return r
    s2["r_sil"] = ranks(s2["sil"].values, True)
    s2["r_ch"]  = ranks(s2["ch"].values, True)
    s2["r_db"]  = ranks(s2["db_inv"].values, True)
    s2["rank_mean"] = (s2["r_sil"] + s2["r_ch"] + s2["r_db"]) / 3.0
    s2 = s2.sort_values(["rank_mean", "sil"], ascending=[True, False])

    # å‡ºåŠ›
    run_dir.mkdir(parents=True, exist_ok=True)
    s2.to_csv(run_dir / "k_candidates_stage2.csv", index=False, encoding="utf-8-sig")
    assign_df.to_csv(run_dir / "k_candidates_assignments.csv", index=False, encoding="utf-8-sig")
    log.info("å€™è£œï¼ˆâ‘¡æœ€çµ‚ï¼‰è©•ä¾¡ã‚’å‡ºåŠ›: %s", run_dir / "k_candidates_stage2.csv")
    log.info("å€™è£œï¼ˆâ‘¡æœ€çµ‚ï¼‰å‰²å½“ã‚’å‡ºåŠ›: %s", run_dir / "k_candidates_assignments.csv")

# ------------- åŸºæº–ã®ä¿å­˜/èª­è¾¼ï¼ˆhistoryç®¡ç†ï¼‰ -------------
def save_baseline_version(basedir: Path, meta1: Dict[str, Any], meta2: Dict[str, Any],
                          centroids_normalized: np.ndarray, info: Dict[str, Any]) -> str:
    hist = basedir / "history"
    hist.mkdir(parents=True, exist_ok=True)
    existing = sorted([p.name for p in hist.glob("v*") if p.is_dir()])
    next_idx = (int(existing[-1][1:]) + 1) if existing else 1
    ver = f"v{next_idx:03d}"
    vdir = hist / ver
    vdir.mkdir(parents=True, exist_ok=True)
    with open(vdir / "baseline_axes_stage1.json", "w", encoding="utf-8") as f:
        json.dump(meta1, f, ensure_ascii=False, indent=2)
    with open(vdir / "baseline_axes_stage2.json", "w", encoding="utf-8") as f:
        json.dump(meta2, f, ensure_ascii=False, indent=2)
    np.save(vdir / "baseline_centroids.npy", centroids_normalized.astype(np.float32))
    with open(vdir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    ledger = basedir / "baseline_manifest.json"
    man = []
    if ledger.exists():
        try:
            man = json.load(open(ledger, "r", encoding="utf-8"))
            if not isinstance(man, list): man = []
        except Exception:
            man = []
    cent_sha = sha256_of_file(vdir / "baseline_centroids.npy")
    man.append(dict(version=ver, sha256=cent_sha, info=info))
    with open(ledger, "w", encoding="utf-8") as f:
        json.dump(man, f, ensure_ascii=False, indent=2)
    return ver

def list_versions(basedir: Path) -> List[str]:
    hist = basedir / "history"
    if not hist.exists(): return []
    return sorted([p.name for p in hist.glob("v*") if p.is_dir()])

def load_baseline_version(basedir: Path, version: Optional[str] = None) -> Tuple[Dict[str, Any], Dict[str, Any], np.ndarray, Dict[str, Any], str]:
    vers = list_versions(basedir)
    if not vers:
        raise FileNotFoundError("baselineã®historyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆå›å®Ÿè¡ŒãŒå¿…è¦ã§ã™ã€‚")
    ver = version if (version and version in vers) else vers[-1]
    vdir = basedir / "history" / ver
    meta1 = json.load(open(vdir / "baseline_axes_stage1.json", "r", encoding="utf-8"))
    meta2 = json.load(open(vdir / "baseline_axes_stage2.json", "r", encoding="utf-8"))
    cent = np.load(vdir / "baseline_centroids.npy")
    info = json.load(open(vdir / "manifest.json", "r", encoding="utf-8"))
    norms = np.linalg.norm(cent, axis=1)
    if not np.allclose(norms, 1.0, atol=1e-3):
        cent = l2_normalize(cent)
    return meta1, meta2, cent, info, ver

# ------------- æŸ”è»Ÿé©ç”¨ï¼ˆæ–°è©±é¡Œã ã‘è¿½åŠ ï¼‰ -------------
def flexible_extend_centroids(Xi2_norm: np.ndarray, base_centroids_norm: np.ndarray,
                              q: float = 0.90, add_k: int = 2, random_state: int = 42) -> Tuple[np.ndarray, Dict[str, Any]]:
    D_base = cosine_distances(Xi2_norm, base_centroids_norm)
    mind = D_base.min(axis=1)
    thr = float(np.quantile(mind, q))
    mask = mind >= thr
    n_new = int(mask.sum())
    info = dict(outlier_threshold=thr, outlier_count=n_new, q=q, add_k=add_k)
    if n_new == 0 or add_k <= 0:
        return base_centroids_norm, info
    sub = Xi2_norm[mask]
    k_eff = int(min(add_k, max(1, len(sub))))
    km = KMeans(n_clusters=k_eff, n_init=10, random_state=random_state)
    _ = km.fit_predict(sub)
    C_new = l2_normalize(km.cluster_centers_.astype(np.float32))
    C_all = np.vstack([base_centroids_norm, C_new]).astype(np.float32)
    info.update(dict(added_clusters=int(k_eff)))
    return C_all, info

# ------------- ãƒ¡ã‚¤ãƒ³ -------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input_xlsx", type=str, default=None)
    ap.add_argument("--input_csv", type=str, default=None)
    ap.add_argument("--text_col", type=str, default=None)
    ap.add_argument("--id_col", type=str, default=None)
    ap.add_argument("--project", type=str, default=None)

    ap.add_argument("--embedding_model", type=str, default="cl-nagoya/ruri-v3-310m")
    ap.add_argument("--batch", type=int, default=16)
    ap.add_argument("--max_len", type=int, default=384)

    ap.add_argument("--pca_var", type=float, default=0.90)
    ap.add_argument("--k_min", type=int, default=3)
    ap.add_argument("--k_max", type=int, default=12)
    ap.add_argument("--random_state", type=int, default=42)

    # å‹•ä½œãƒ¢ãƒ¼ãƒ‰
    ap.add_argument("--unlock", dest="unlock", action="store_true", help="æŸ”è»Ÿé©ç”¨ï¼ˆåŸºæº–æº–æ‹ ï¼‹æ–°è©±é¡Œã ã‘è¿½åŠ ã—ã¦ç‰ˆè¿½åŠ ä¿å­˜ï¼‰")
    ap.add_argument("--æŸ”è»Ÿé©ç”¨", dest="unlock", action="store_true")
    ap.add_argument("--show-candidates", dest="show_candidates", action="store_true", help="å€™è£œå‡ºã—ã®ã¿ï¼ˆâ‘¡æœ€çµ‚ã§æ¯”è¼ƒç”¨CSVã‚‚å‡ºåŠ›ï¼‰")
    ap.add_argument("--å€™è£œè¡¨ç¤º", dest="show_candidates", action="store_true")
    ap.add_argument("--use-plan", dest="use_plan", type=int, default=None, help="åˆå›ã®åŸºæº–ä½œæˆæ™‚ã«æ¡ç”¨ã™ã‚‹Planç•ªå·")
    ap.add_argument("--æ¡ç”¨ãƒ—ãƒ©ãƒ³", dest="use_plan", type=int, default=None)
    ap.add_argument("--baseline-from", dest="baseline_from", type=str, default=None, help="å¤–éƒ¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åŸºæº–ã‚’æµç”¨ã—ã¦ãƒ­ãƒƒã‚¯/æŸ”è»Ÿé©ç”¨ï¼ˆæœ€å„ªå…ˆï¼‰")
    ap.add_argument("--åŸºæº–æµç”¨", dest="baseline_from", type=str, default=None)

    # æŸ”è»Ÿé©ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    ap.add_argument("--unlock-q", dest="unlock_q", type=float, default=0.90, help="å¤–ã‚Œå€¤åˆ¤å®šã®è·é›¢åˆ†ä½ç‚¹ï¼ˆ0..1ï¼‰")
    ap.add_argument("--unlock-add-k", dest="unlock_add_k", type=int, default=2, help="æ–°è¦ã«è¿½åŠ ã™ã‚‹æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°")

    ap.add_argument("--max_ic_cols", type=int, default=None, help="CSVã«å‡ºã™ICï¼ˆICAâ‘¡ï¼‰åˆ—ã®ä¸Šé™ï¼ˆå†…éƒ¨ã¯ãƒ•ãƒ«ï¼‰")
    ap.add_argument("--log_level", type=str, default="INFO")
    args = ap.parse_args()
    setup_logging(args.log_level)

    if not (0.0 < float(args.unlock_q) < 1.0):
        log.error("--unlock-q ã¯ (0, 1) ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹: 0.90")
        sys.exit(2)

    np.random.seed(args.random_state)

    # å…¥åŠ›
    if args.input_xlsx:
        infile, ext = Path(args.input_xlsx), "xlsx"
    elif args.input_csv:
        infile, ext = Path(args.input_csv), "csv"
    else:
        infile, ext = autodetect_input()

    df0 = read_table(infile, ext)
    text_col, id_col = autodetect_columns(df0, args.text_col, args.id_col)
    log.info('columns: text_col="%s"%s', text_col, f', id_col="{id_col}"' if id_col else "")
    df = df0[[c for c in [id_col, text_col] if c is not None]].copy()
    if id_col is None:
        df["id"] = np.arange(len(df))
        df.rename(columns={text_col: "text"}, inplace=True)
        keep_cols = ["id", "text"]
    else:
        df.rename(columns={text_col: "text", id_col: "id"}, inplace=True)
        keep_cols = ["id", "text"]
    df["text"] = df["text"].astype(str).map(normalize_text)
    n = len(df)
    log.info("ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: %d", n)

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    project = get_project_name(args.project, infile)
    result_root = Path("PVMresult")
    baseline_dir_current = result_root / f"baseline_{project}"
    has_current_baseline = len(list_versions(baseline_dir_current)) > 0

    # ä½¿ã† baseline ã®æ±ºå®šï¼ˆåŸ‹ã‚è¾¼ã¿å‰ã®äºˆå‘Šãƒ­ã‚°ï¼‰
    selected_baseline_project: Optional[str] = None
    selected_baseline_ver: Optional[str] = None

    if args.baseline_from:
        cand_dir = result_root / f"baseline_{args.baseline_from}"
        if not cand_dir.exists() or not list_versions(cand_dir):
            raise FileNotFoundError(f"--baseline-from ã§æŒ‡å®šã—ãŸåŸºæº–ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {cand_dir}")
        selected_baseline_project = args.baseline_from
        selected_baseline_ver = list_versions(cand_dir)[-1]
        mode_hint = "ğŸ”“ æŸ”è»Ÿé©ç”¨äºˆå®š" if args.unlock else "ğŸ”’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ­ãƒƒã‚¯äºˆå®š"
        log.info("%sï¼šå¤–éƒ¨åŸºæº–ã‚’ä½¿ç”¨ï¼ˆbaseline: %s, ver: %sï¼‰", mode_hint, selected_baseline_project, selected_baseline_ver)
    elif has_current_baseline:
        selected_baseline_project = project
        selected_baseline_ver = list_versions(baseline_dir_current)[-1]
        mode_hint = "ğŸ”“ æŸ”è»Ÿé©ç”¨äºˆå®š" if args.unlock else "ğŸ”’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ­ãƒƒã‚¯äºˆå®š"
        log.info("%sï¼šå‰å›åŸºæº–ã‚’ä½¿ç”¨ï¼ˆbaseline: %s, ver: %sï¼‰", mode_hint, selected_baseline_project, selected_baseline_ver)
    else:
        latest = find_latest_baseline_project(result_root)
        if latest is not None:
            selected_baseline_project = latest
            latest_dir = result_root / f"baseline_{latest}"
            selected_baseline_ver = list_versions(latest_dir)[-1]
            mode_hint = "ğŸ”“ æŸ”è»Ÿé©ç”¨äºˆå®š" if args.unlock else "ğŸ”’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ­ãƒƒã‚¯äºˆå®š"
            log.info("%sï¼šè‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸåŸºæº–ã‚’ä½¿ç”¨ï¼ˆbaseline: %s, ver: %sï¼‰", mode_hint, selected_baseline_project, selected_baseline_ver)
        else:
            # å…¨ãåŸºæº–ãŒãªã„ â†’ åˆå›ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹
            if args.show_candidates:
                log.info("ğŸ§­ åˆå›ï¼ˆå€™è£œå‡ºã—ã®ã¿ï¼‰ï¼šã“ã®å›ã§ã¯åŸºæº–ã‚’ä½œã‚Šã¾ã›ã‚“ã€‚`k_candidates.csv` ã¨ â‘¡æœ€çµ‚ç”¨CSV ã‚’å‡ºã—ã¾ã™ã€‚")
            elif args.use_plan is not None:
                log.info("ğŸ§­ åˆå›ï¼ˆåŸºæº–ä½œæˆï¼‰ï¼šé¸æŠæ¸ˆã¿ã® Plan #%d ã§åŸºæº–ã‚’ä½œæˆã—ã¾ã™ã€‚", int(args.use_plan))
            else:
                log.info("ğŸ§­ åˆå›ï¼ˆè‡ªå‹•åŸºæº–ä½œæˆï¼‰ï¼šãƒ™ã‚¹ãƒˆPlanã‚’è‡ªå‹•æ¡ç”¨ã—ã¦åŸºæº–ã‚’ä½œæˆã—ã¾ã™ã€‚å€™è£œã ã‘è¦‹ãŸã„å ´åˆã¯ `--show-candidates` ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚")

    if args.unlock and selected_baseline_project is None:
        log.error("æŸ”è»Ÿé©ç”¨ï¼ˆ--unlockï¼‰ã«ã¯æ—¢å­˜ã®åŸºæº–ãŒå¿…è¦ã§ã™ã€‚ã¾ãšåˆå›å®Ÿè¡Œã§åŸºæº–ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        sys.exit(2)

    log.info("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: %s", project)

    # Ruriãƒã‚§ãƒƒã‚¯ & Embedding
    if not ensure_ruri():
        sys.exit(1)
    X, device = compute_embeddings(df["text"].tolist(), args.embedding_model, args.batch, args.max_len)

    if n < max(30, args.k_max * 3):
        log.warning("ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãŒå°‘ãªã‚ã§ã™ï¼ˆn=%dï¼‰ã€‚k_max=%d ã¯ç²—ã‚ã®æ¢ç´¢ã«ãªã‚Šã¾ã™ã€‚", n, args.k_max)

    # ====== åˆ†å² ======
    if selected_baseline_project is None and not args.unlock:
        # === åˆå›ãƒ•ãƒ­ãƒ¼ï¼ˆåŸºæº–ãªã—ï¼‰ ===
        log.info("=== å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: åˆå› ===")

        # å‡ºåŠ›å…ˆ run_dir
        run_idx = 1
        while True:
            run_dir = result_root / f"run_{project}_{run_idx:02d}"
            if not run_dir.exists():
                break
            run_idx += 1
        run_dir.mkdir(parents=True, exist_ok=True)

        # å€™è£œæ¢ç´¢ï¼ˆâ‘ ï¼šd Ã— Kï¼‰
        cands, extra = explore_candidates(X, args.k_min, args.k_max, args.pca_var, args.random_state)
        if not cands:
            raise RuntimeError("æœ‰åŠ¹ãªå€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")

        export_candidates(run_dir, cands)

        # Stage2ç”¨ã®è£œåŠ©ï¼ˆd*å›ºå®šã§Kã ã‘å¤‰ãˆã‚‹TOP5ï¼‰
        d_star = choose_best_d_by_rank(cands)
        top5_for_stage2 = pick_top5_for_d(cands, d_star)

        if args.show_candidates:
            # â‘¡ã®æœ€çµ‚çµæœã§æ¯”è¼ƒã§ãã‚‹CSVã‚’å‡ºåŠ›
            export_stage2_selection(run_dir, df, keep_cols, X, args.pca_var, d_star,
                                    top5_for_stage2, args.random_state, args.max_ic_cols)
            # ãƒ­ã‚°æ•´å½¢
            log.info("å€™è£œï¼ˆTOP5 / d*=%d å›ºå®šãƒ»Kã ã‘æ¯”è¼ƒ / ãƒ©ãƒ³ã‚¯å¹³å‡ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼‰:", d_star)
            log.info("   Plan  K    sil     CH     DB_inv")
            tmp = pd.read_csv(run_dir / "k_candidates_stage2.csv")
            for _, row in tmp.iterrows():
                log.info("   %4d %3d  %5.3f %6.0f   %5.3f",
                         int(row['plan']), int(row['k']),
                         float(row['sil']), float(row['ch']), float(row['db_inv']))
            log.info("ã“ã®å›ã¯å€™è£œå‡ºã—ã§çµ‚äº†ã—ã¾ã™ã€‚`k_candidates_stage2.csv` / `k_candidates_assignments.csv` ã‚’è¦‹ã¦ã€")
            log.info("æ¬¡ã¯ `--use-plan N`ï¼ˆä»»æ„ï¼‰ã‹æœªæŒ‡å®šã§ãƒ™ã‚¹ãƒˆæ¡ç”¨ã«ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            log.info("ğŸ‘‰ ä¾‹: `python PVM.py --use-plan 3`ï¼ˆPlan#3ã‚’æ¡ç”¨ï¼‰ / `python PVM.py`ï¼ˆãƒ™ã‚¹ãƒˆPlanã‚’è‡ªå‹•æ¡ç”¨ï¼‰")
            return

        # Planæ±ºå®šï¼ˆæœªæŒ‡å®šãªã‚‰ rank=1ï¼‰
        plan_idx = (max(1, min(int(args.use_plan), len(cands))) - 1) if (args.use_plan is not None) else 0
        chosen = cands[plan_idx]
        d_star, k_star = int(chosen["ica1_dim"]), int(chosen["k"])
        log.info("åŸºæº–ä½œæˆ: Plan #%d ã‚’æ¡ç”¨ â†’ d=%d, K=%d", plan_idx+1, d_star, k_star)

        # Stage1 & Stage2
        meta1, Xi1 = fit_stage1_axes(X, pca_var=args.pca_var, ica1_dim=d_star, random_state=args.random_state)
        meta2, Xi2 = fit_stage2_axes(Xi1, k_star, random_state=args.random_state)
        Xi2_norm = l2_normalize(Xi2)
        km2 = KMeans(n_clusters=k_star, n_init=20, random_state=args.random_state)
        labels2 = km2.fit_predict(Xi2_norm)
        C2_norm = l2_normalize(km2.cluster_centers_.astype(np.float32))
        D = cosine_distances(Xi2_norm, C2_norm)
        labels_final = np.argmin(D, axis=1).astype(int)
        dists = D[np.arange(len(D)), labels_final]

        # baselineä¿å­˜
        baseline_dir_current.mkdir(parents=True, exist_ok=True)
        info = dict(
            project=project,
            plan=dict(rank=plan_idx+1, ica1_dim=int(meta1["ica1_n_components"]), k=k_star, chosen=chosen),
            script_version=SCRIPT_VERSION,
            embedding_model=args.embedding_model,
            pca_var=args.pca_var,
            random_state=args.random_state,
            rank_method="mean_of_ranks(silâ†‘, CHâ†‘, 1/(1+DB)â†‘)",
            metric_family="cosine-spherical",
            centroids_are_normalized=True,
            mode="first",
            environment=dict(
                python=platform.python_version(),
                os=platform.platform(),
            )
        )
        try:
            import torch, transformers, sklearn  # type: ignore
            info["environment"].update(dict(
                torch=torch.__version__,
                transformers=transformers.__version__,
                sklearn=sklearn.__version__,
                device=device,
            ))
        except Exception:
            pass

        ver = save_baseline_version(baseline_dir_current, meta1, meta2, C2_norm, info)

        export_run_csv(run_dir, df, keep_cols, Xi2, labels_final, dists, args.max_ic_cols)
        export_report(run_dir, dict(
            n=n, project=project, mode="first (commit)", used_version=ver,
            ica1_dim=int(meta1["ica1_n_components"]), k=k_star, ic2_dim=int(meta2["ica2_n_components"]),
            candidates_top5=pick_top5_for_d(cands, choose_best_d_by_rank(cands))[:5],
            rank_method="mean_of_ranks(silâ†‘, CHâ†‘, 1/(1+DB)â†‘)",
            metric_family="cosine-spherical",
        ))
        export_naming_prompt(run_dir, df, text_col, labels_final, dists, top_n=5, include_dist=True)
        log.info("baselineä½œæˆ: %s", baseline_dir_current / "history" / ver)
        log.info("=========== Summary ===========")
        log.info("n=%d, project=%s, outdir=%s", n, project, str(run_dir.resolve()))
        log.info("å®Œäº†ã€‚")
        return

    # === baseline ã‚ã‚Šï¼ˆãƒ­ãƒƒã‚¯ or æŸ”è»Ÿé©ç”¨ï¼‰ ===
    run_idx = 1
    while True:
        run_dir = result_root / f"run_{project}_{run_idx:02d}"
        if not run_dir.exists():
            break
        run_idx += 1
    run_dir.mkdir(parents=True, exist_ok=True)

    base_dir = result_root / f"baseline_{selected_baseline_project}"
    meta1, meta2, C2_norm, info, ver = load_baseline_version(base_dir, version=None)
    if info.get("embedding_model") and info["embedding_model"] != args.embedding_model:
        log.warning("baselineã®embedding_model(%s) ã¨ç¾åœ¨(%s) ãŒä¸ä¸€è‡´ã€‚æ™‚ç³»åˆ—æ¯”è¼ƒã®å³å¯†æ€§ãŒä¸‹ãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
                    info["embedding_model"], args.embedding_model)

    Xi1 = apply_stage1_axes(X, meta1)
    Xi2 = apply_stage2_axes(Xi1, meta2)
    Xi2_norm = l2_normalize(Xi2)

    if args.unlock:
        log.info("=== å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æŸ”è»Ÿé©ç”¨ï¼ˆåŸºæº–æº–æ‹ ï¼‹æ–°è©±é¡Œå¸åï¼‰ ===")
        C_all, flex_info = flexible_extend_centroids(
            Xi2_norm, C2_norm, q=float(args.unlock_q), add_k=int(args.unlock_add_k),
            random_state=args.random_state
        )
        D_all = cosine_distances(Xi2_norm, C_all)
        labels_final = np.argmin(D_all, axis=1).astype(int)
        dists = D_all[np.arange(len(D_all)), labels_final]

        base_mind = cosine_distances(Xi2_norm, C2_norm).min(axis=1)
        extra_cols = {"flex_added": (base_mind >= flex_info["outlier_threshold"]).astype(int)}

        baseline_dir_current.mkdir(parents=True, exist_ok=True)
        info2 = dict(info)
        info2.update(dict(
            project=project,
            mode="unlock",
            unlock_params=flex_info,
            script_version=SCRIPT_VERSION,
        ))
        ver2 = save_baseline_version(baseline_dir_current, meta1, meta2, C_all, info2)

        export_run_csv(run_dir, df, keep_cols, Xi2, labels_final, dists, args.max_ic_cols, extra_cols=extra_cols)
        export_report(run_dir, dict(
            n=n, project=project, mode="unlock (commit)", used_version=ver2,
            base_version_used=f"{base_dir.name}:{ver}",
            ica1_dim=int(meta1["ica1_n_components"]), k=int(C_all.shape[0]),
            ic2_dim=int(meta2["ica2_n_components"]),
            unlock_params=flex_info,
            metric_family=info.get("metric_family", "cosine-spherical"),
            centroids_are_normalized=True,
        ))
        export_naming_prompt(run_dir, df, text_col, labels_final, dists, top_n=5, include_dist=True)
        log.info("æŸ”è»Ÿé©ç”¨: æ–°è©±é¡Œå€™è£œ = %d ä»¶ï¼ˆq=%.2fï¼‰", flex_info.get("outlier_count", 0), float(args.unlock_q))
        log.info("æŸ”è»Ÿé©ç”¨: è¿½åŠ ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ = %d å€‹", flex_info.get("added_clusters", 0))
        log.info("æŸ”è»Ÿé©ç”¨: è¿½åŠ å¾Œã®åŸºæº–ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ %s", baseline_dir_current / "history" / ver2)

    else:
        log.info("=== å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ­ãƒƒã‚¯ï¼ˆåŸºæº–å›ºå®š: %s, ver: %sï¼‰ ===", base_dir.name, ver)
        D = cosine_distances(Xi2_norm, C2_norm)
        labels_final = np.argmin(D, axis=1).astype(int)
        dists = D[np.arange(len(D)), labels_final]
        export_run_csv(run_dir, df, keep_cols, Xi2, labels_final, dists, args.max_ic_cols)
        export_report(run_dir, dict(
            n=n, project=project, mode="lock", used_version=ver,
            base_project=base_dir.name.replace("baseline_", "", 1),
            ica1_dim=int(meta1["ica1_n_components"]), k=int(C2_norm.shape[0]),
            ic2_dim=int(meta2["ica2_n_components"]),
            metric_family=info.get("metric_family", "cosine-spherical"),
            centroids_are_normalized=info.get("centroids_are_normalized", True),
        ))
        export_naming_prompt(run_dir, df, text_col, labels_final, dists, top_n=5, include_dist=True)
        log.info("=========== Summary ===========")
        log.info("n=%d, project=%s, outdir=%s", n, project, str(run_dir.resolve()))
        log.info("å®Œäº†ã€‚")

if __name__ == "__main__":
    main()